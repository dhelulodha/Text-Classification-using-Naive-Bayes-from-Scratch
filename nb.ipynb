{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sources):\n",
    "    \"\"\"\n",
    "    inputs a list of all filepaths\n",
    "    outputs 4 lists:\n",
    "    x1 - file content\n",
    "    y1 - label corresponding to x1 (positive/negative)\n",
    "    x2 - file content\n",
    "    y2 - label corresponding to x2 (truthful/deceptive)\n",
    "    \"\"\"\n",
    "    x1=[]\n",
    "    y1=[]\n",
    "    x2=[]\n",
    "    y2=[]\n",
    "    for src in sources:\n",
    "        f = open(src, \"r\")\n",
    "        file_content = f.read()[:-1]\n",
    "        x1.append([file_content, src])\n",
    "        x2.append([file_content, src])\n",
    "        f.close()\n",
    "        if src.split(\"/\")[-4]==\"negative_polarity\":\n",
    "            y1.append(0)\n",
    "        else:\n",
    "            y1.append(1)\n",
    "        if src.split(\"/\")[-3]==\"deceptive_from_MTurk\":\n",
    "            y2.append(0)\n",
    "        else:\n",
    "            y2.append(1)\n",
    "    return x1,y1,x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training datasets\n",
    "train_base_path = 'D:/USC/Applied Natural Language Processing - 544/Naive_Bayes/op_spam_training_data/train/**/*.txt'\n",
    "train_reviews = glob.glob(train_base_path,recursive=True)\n",
    "train_reviews = [review.replace(\"\\\\\",\"/\") for review in train_reviews]\n",
    "x1_train,y1_train,x2_train,y2_train = create_dataset(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"len of x1_train: \",len(x1_train))\n",
    "# print(x1_train[0])\n",
    "# print(\"len of y1_train: \",len(y1_train))\n",
    "# print(y1_train[0])\n",
    "# print(\"len of x2_train: \",len(x2_train))\n",
    "# print(x2_train[0])\n",
    "# print(\"len of y2_train: \",len(y2_train))\n",
    "# print(y2_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation datasets\n",
    "valid_base_path = 'D:/USC/Applied Natural Language Processing - 544/Naive_Bayes/op_spam_training_data/validation/**/*.txt'\n",
    "valid_reviews = glob.glob(valid_base_path,recursive=True)\n",
    "valid_reviews = [review.replace(\"\\\\\",\"/\") for review in valid_reviews]\n",
    "x1_valid,y1_valid,x2_valid,y2_valid = create_dataset(valid_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words that should be removed (no contribution to prediction, computation)\n",
    "stop_words = [ 'are', 'around','as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "             'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who',\n",
    "             'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "             'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "             'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "             'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    "             'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "             'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
    "             'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd',\n",
    "             'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',\n",
    "             \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
    "             'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "\n",
    "# preprocess string function (stemming lementization to be added)\n",
    "\n",
    "def preprocess_string(s,stop_words):\n",
    "    s=s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s=re.sub('(\\s+)',' ',s)\n",
    "    s=s.lower()\n",
    "    word_list = s.split(\" \")\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        if (len(word)>2)  and (word not in stop_words):  \n",
    "            new_list.append (word)\n",
    "    s=\" \".join(new_list)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all words list\n",
    "# vocab_positive_negative = {}\n",
    "# for i in range(len(x1_train)):\n",
    "#     for word in x1_train[i][0].split(\" \"):\n",
    "#         word  = word.strip(string.punctuation).lower()\n",
    "#         if (len(word)>2)  and (word not in stop_words):  \n",
    "#             if word in vocab_positive_negative:\n",
    "#                 vocab_positive_negative[word]+=1\n",
    "#             else:\n",
    "#                 vocab_positive_negative[word]=1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all words list\n",
    "vocab_positive_negative = {}\n",
    "for i in range(len(x1_train)):\n",
    "    x1_train[i][0]=x1_train[i][0].translate(str.maketrans('', '', string.punctuation))\n",
    "    x1_train[i][0]=x1_train[i][0].lower()\n",
    "    word_list = x1_train[i][0].split(\" \")\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        if (len(word)>2)  and (word not in stop_words):  \n",
    "            if word in vocab_positive_negative:\n",
    "                vocab_positive_negative[word]+=1\n",
    "            else:\n",
    "                vocab_positive_negative[word]=1\n",
    "            new_list.append (word)\n",
    "    x1_train[i][0]=\" \".join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x1_train:  960\n",
      "[\"Affinia Chicago is one of the worst hotels I have ever stayed at. Not in my life have I been treated so poorly as a guest. The front desk was very unaccommodating when I asked for a smoke free room when they had made an error in my reservation. There was no bellhop available for some strange reason so I had to move all my luggage to the elevator and down a long hallway to my room by myself. If it wasn't already a bad stay, I ordered room service and it took over an hour and a half to be delivered. If they didn't have air conditioning in the room, I would say just about everything about this stay was completely miserable. If you are traveling to Chicago for any kind of business, I hope you decide not to choose this hotel. I was quite surprised, I like Chicago as a city but this stay definitely made my trip quite a negative experience.\", 'D:/USC/Applied Natural Language Processing - 544/Naive_Bayes/op_spam_training_data/train/negative_polarity/deceptive_from_MTurk/fold2/d_affinia_1.txt']\n",
      "***\n",
      "affinia chicago worst hotels stayed life treated poorly guest desk unaccommodating asked smoke free room error reservation bellhop available strange reason luggage elevator long hallway room wasnt already bad stay ordered room service took hour half delivered didnt air conditioning room say stay completely miserable traveling chicago kind business hope decide choose hotel quite surprised like chicago city stay definitely trip quite negative experience\n"
     ]
    }
   ],
   "source": [
    "print(\"len of x1_train: \",len(x1_train))\n",
    "print(x1_train[0])\n",
    "\n",
    "print(\"***\")\n",
    "\n",
    "print(preprocess_string(x1_train[0][0],stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_positive_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import heapq\n",
    "# most_freq = heapq.nlargest(5000, vocab_positive_negative, key=vocab_positive_negative.get)\n",
    "# most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(list(vocab_positive_negative.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_freq=2\n",
    "features = []\n",
    "for key in vocab_positive_negative:\n",
    "    if vocab_positive_negative[key] >= threshold_freq:\n",
    "        features.append(key)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train_dataset = [([1]*len(features)) for i in range(len(x1_train))]\n",
    "\n",
    "for i in range(len(x1_train)):\n",
    "    word_list = x1_train[i][1].split(\" \")\n",
    "    for word in word_list:\n",
    "        if word in features:\n",
    "            x1_train_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_valid_dataset = [([1]*len(features)) for i in range(len(x1_valid))]\n",
    "\n",
    "for i in range(len(x1_valid)):\n",
    "    word_list = x1_valid[i][1].split(\" \")\n",
    "    for word in word_list:\n",
    "        if word in features:\n",
    "            x1_valid_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NaiveBayes:\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         self._classes = np.unique(y)\n",
    "#         n_classes = len(self._classes)\n",
    "\n",
    "#         # calculate mean, var, and prior for each class\n",
    "#         self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "#         self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "#         self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "#         for idx, c in enumerate(self._classes):\n",
    "#             X_c = X[y == c]\n",
    "#             self._mean[idx, :] = X_c.mean(axis=0)\n",
    "#             self._var[idx, :] = X_c.var(axis=0)\n",
    "#             self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         y_pred = [self._predict(x) for x in X]\n",
    "#         return np.array(y_pred)\n",
    "\n",
    "#     def _predict(self, x):\n",
    "#         posteriors = []\n",
    "\n",
    "#         # calculate posterior probability for each class\n",
    "#         for idx, c in enumerate(self._classes):\n",
    "#             prior = np.log(self._priors[idx])\n",
    "#             posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "#             posterior = prior + posterior\n",
    "#             posteriors.append(posterior)\n",
    "\n",
    "#         # return class with highest posterior probability\n",
    "#         return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "#     def _pdf(self, class_idx, x):\n",
    "#         mean = self._mean[class_idx]\n",
    "#         var = self._var[class_idx]\n",
    "#         numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "#         denominator = np.sqrt(2 * np.pi * var)\n",
    "#         return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(y_true, y_pred):\n",
    "#         accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "#         return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb = NaiveBayes()\n",
    "# nb.fit(np.asarray(x1_train_dataset), np.asarray(y1_train))\n",
    "# predictions = nb.predict(np.asarray(x1_valid_dataset))\n",
    "\n",
    "# print(\"Naive Bayes classification accuracy\", accuracy(y1_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
