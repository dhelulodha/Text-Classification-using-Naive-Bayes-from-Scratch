{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sources):\n",
    "    \"\"\"\n",
    "    inputs a list of all filepaths\n",
    "    outputs 4 lists:\n",
    "    x1 - file content\n",
    "    y1 - label corresponding to x1 (positive/negative)\n",
    "    x2 - file content\n",
    "    y2 - label corresponding to x2 (truthful/deceptive)\n",
    "    \"\"\"\n",
    "    x1=[]\n",
    "    y1=[]\n",
    "    x2=[]\n",
    "    y2=[]\n",
    "    for src in sources:\n",
    "        f = open(src, \"r\")\n",
    "        file_content = f.read()[:-1]\n",
    "        x1.append(file_content)\n",
    "        x2.append(file_content)\n",
    "        f.close()\n",
    "        if src.split(\"/\")[-4]==\"negative_polarity\":\n",
    "            y1.append(0)\n",
    "        else:\n",
    "            y1.append(1)\n",
    "        if src.split(\"/\")[-3]==\"deceptive_from_MTurk\":\n",
    "            y2.append(0)\n",
    "        else:\n",
    "            y2.append(1)\n",
    "    return x1,y1,x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training datasets\n",
    "train_base_path = 'D:/USC/Applied Natural Language Processing - 544/Naive_Bayes/op_spam_training_data/train/**/*.txt'\n",
    "train_reviews = glob.glob(train_base_path,recursive=True)\n",
    "train_reviews = [review.replace(\"\\\\\",\"/\") for review in train_reviews]\n",
    "x1_train,y1_train,x2_train,y2_train = create_dataset(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation datasets\n",
    "valid_base_path = 'D:/USC/Applied Natural Language Processing - 544/Naive_Bayes/op_spam_training_data/validation/**/*.txt'\n",
    "valid_reviews = glob.glob(valid_base_path,recursive=True)\n",
    "valid_reviews = [review.replace(\"\\\\\",\"/\") for review in valid_reviews]\n",
    "x1_valid,y1_valid,x2_valid,y2_valid = create_dataset(valid_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocess string function (stemming lementization to be added)\n",
    "\n",
    "def preprocess_string(s):\n",
    "    \n",
    "    # words that should be removed (no contribution to prediction, computation)\n",
    "    stop_words = [ 'are', 'around','as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "             'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who',\n",
    "             'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "             'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "             'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "             'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    "             'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "             'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
    "             'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd',\n",
    "             'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',\n",
    "             \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
    "             'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren',\n",
    "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "    \n",
    "    s=s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s=re.sub('(\\s+)',' ',s)\n",
    "    s=s.lower()\n",
    "    word_list = s.split(\" \")\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        if (len(word)>2)  and (word not in stop_words):  \n",
    "            new_list.append (word)\n",
    "    s=\" \".join(new_list)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all words list\n",
    "# vocab_positive_negative = {}\n",
    "# for i in range(len(x1_train)):\n",
    "#     for word in x1_train[i][0].split(\" \"):\n",
    "#         word  = word.strip(string.punctuation).lower()\n",
    "#         if (len(word)>2)  and (word not in stop_words):  \n",
    "#             if word in vocab_positive_negative:\n",
    "#                 vocab_positive_negative[word]+=1\n",
    "#             else:\n",
    "#                 vocab_positive_negative[word]=1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all words list\n",
    "# vocab_positive_negative = {}\n",
    "# for i in range(len(x1_train)):\n",
    "#     x1_train[i][0]=x1_train[i][0].translate(str.maketrans('', '', string.punctuation))\n",
    "#     x1_train[i][0]=x1_train[i][0].lower()\n",
    "#     word_list = x1_train[i][0].split(\" \")\n",
    "#     new_list = []\n",
    "#     for word in word_list:\n",
    "#         if (len(word)>2)  and (word not in stop_words):  \n",
    "#             if word in vocab_positive_negative:\n",
    "#                 vocab_positive_negative[word]+=1\n",
    "#             else:\n",
    "#                 vocab_positive_negative[word]=1\n",
    "#             new_list.append (word)\n",
    "#     x1_train[i][0]=\" \".join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x1_train:  960\n",
      "Affinia Chicago is one of the worst hotels I have ever stayed at. Not in my life have I been treated so poorly as a guest. The front desk was very unaccommodating when I asked for a smoke free room when they had made an error in my reservation. There was no bellhop available for some strange reason so I had to move all my luggage to the elevator and down a long hallway to my room by myself. If it wasn't already a bad stay, I ordered room service and it took over an hour and a half to be delivered. If they didn't have air conditioning in the room, I would say just about everything about this stay was completely miserable. If you are traveling to Chicago for any kind of business, I hope you decide not to choose this hotel. I was quite surprised, I like Chicago as a city but this stay definitely made my trip quite a negative experience.\n",
      "***\n",
      "affinia chicago worst hotels stayed life treated poorly guest desk unaccommodating asked smoke free room error reservation bellhop available strange reason luggage elevator long hallway room wasnt already bad stay ordered room service took hour half delivered didnt air conditioning room say stay completely miserable traveling chicago kind business hope decide choose hotel quite surprised like chicago city stay definitely trip quite negative experience\n"
     ]
    }
   ],
   "source": [
    "print(\"len of x1_train: \",len(x1_train))\n",
    "print(x1_train[0])\n",
    "\n",
    "print(\"***\")\n",
    "\n",
    "print(preprocess_string(x1_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(vocab_positive_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import heapq\n",
    "# most_freq = heapq.nlargest(5000, vocab_positive_negative, key=vocab_positive_negative.get)\n",
    "# most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.asarray(list(vocab_positive_negative.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_freq=2\n",
    "# features = []\n",
    "# for key in vocab_positive_negative:\n",
    "#     if vocab_positive_negative[key] >= threshold_freq:\n",
    "#         features.append(key)\n",
    "# print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1_train_dataset = [([1]*len(features)) for i in range(len(x1_train))]\n",
    "\n",
    "# for i in range(len(x1_train)):\n",
    "#     word_list = x1_train[i][1].split(\" \")\n",
    "#     for word in word_list:\n",
    "#         if word in features:\n",
    "#             x1_train_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1_valid_dataset = [([1]*len(features)) for i in range(len(x1_valid))]\n",
    "\n",
    "# for i in range(len(x1_valid)):\n",
    "#     word_list = x1_valid[i][1].split(\" \")\n",
    "#     for word in word_list:\n",
    "#         if word in features:\n",
    "#             x1_valid_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "      \n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "        for token_word in example.split():\n",
    "            self.bow_dicts[dict_index][token_word]+=1\n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "        \n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat] \n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "\n",
    "      \n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
    "                           \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                                              \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "      \n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                \n",
    "        \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) \n",
    "        \n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split():\n",
    "                \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                                   \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                \n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "      \n",
    "        predictions=[] \n",
    "        for example in test_set:                                 \n",
    "            cleaned_example=preprocess_string(example)                  \n",
    "            post_prob=self.getExampleProb(cleaned_example) \n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Training In Progress --------------------\n",
      "----------------- Training Completed ---------------------\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes(np.unique(y1_train)) \n",
    "print (\"Training Started\")\n",
    " \n",
    "nb.train(x1_train,y1_train) \n",
    "print (\"Training Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Examples:  320\n",
      "Test Set Accuracy:  91.875 %\n"
     ]
    }
   ],
   "source": [
    "pclasses=nb.test(x1_valid)\n",
    "test_acc=np.sum(pclasses==y1_valid)/float(np.asarray(y1_valid).shape[0]) \n",
    "print (\"Test Set Examples: \",np.asarray(y1_valid).shape[0])\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
